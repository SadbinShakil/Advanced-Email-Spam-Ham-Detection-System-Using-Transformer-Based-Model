{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g6BI6IOf45E9"
      },
      "source": [
        "**Try to enhance the model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T8xIO6QAeMRF",
        "outputId": "f0979fdc-8b95-445c-9ec7-3e9eda63dd4d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File content preview:\n",
            "     v1                                                 v2 Unnamed: 2  \\\n",
            "0   ham  Go until jurong point, crazy.. Available only ...        NaN   \n",
            "1   ham                      Ok lar... Joking wif u oni...        NaN   \n",
            "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...        NaN   \n",
            "3   ham  U dun say so early hor... U c already then say...        NaN   \n",
            "4   ham  Nah I don't think he goes to usf, he lives aro...        NaN   \n",
            "\n",
            "  Unnamed: 3 Unnamed: 4  \n",
            "0        NaN        NaN  \n",
            "1        NaN        NaN  \n",
            "2        NaN        NaN  \n",
            "3        NaN        NaN  \n",
            "4        NaN        NaN  \n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# Check if the file exists\n",
        "\n",
        "file_path = \"/content/spam.csv\"\n",
        "if os.path.isfile(file_path):\n",
        "    data = pd.read_csv(file_path, encoding=\"latin-1\")\n",
        "    print(\"File content preview:\")\n",
        "    print(data.head())\n",
        "else:\n",
        "    print(f\"The file '{file_path}' does not exist.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5odhK72gK9gK",
        "outputId": "0958a537-093d-45cb-8ba5-b48d1b150961"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset from https://spamassassin.apache.org/old/publiccorpus/20030228_spam.tar.bz2 downloaded and extracted.\n",
            "Dataset from https://spamassassin.apache.org/old/publiccorpus/20030228_easy_ham.tar.bz2 downloaded and extracted.\n",
            "\n",
            "--- SpamAssassin Spam Data ---\n",
            "                                               email  label\n",
            "0  From ilug-admin@linux.ie  Tue Sep 24 15:54:23 ...      1\n",
            "1  Return-Path: ler@lerami.lerctr.org\\nDelivery-D...      1\n",
            "2  From ilug-admin@linux.ie  Mon Sep 16 10:44:18 ...      1\n",
            "3  From tammy490t@yahoo.com  Tue Aug 27 05:38:41 ...      1\n",
            "4  From donaldbae@purplehotel.com  Wed Aug 28 11:...      1\n",
            "email    501\n",
            "label    501\n",
            "dtype: int64\n",
            "\n",
            "--- SpamAssassin Ham Data ---\n",
            "                                               email  label\n",
            "0  From rssfeeds@jmason.org  Thu Oct  3 12:24:54 ...      0\n",
            "1  From pudge@perl.org  Thu Sep 26 11:02:41 2002\\...      0\n",
            "2  From exmh-users-admin@redhat.com  Fri Sep 13 1...      0\n",
            "3  From rssfeeds@jmason.org  Thu Sep 26 16:42:08 ...      0\n",
            "4  From rssfeeds@jmason.org  Mon Sep 30 13:37:07 ...      0\n",
            "email    2501\n",
            "label    2501\n",
            "dtype: int64\n",
            "Loading spam.csv dataset from /content/spam.csv...\n",
            "spam.csv dataset loaded successfully.\n",
            "\n",
            "--- spam.csv Data ---\n",
            "   label                                              email\n",
            "0      0  Go until jurong point, crazy.. Available only ...\n",
            "1      0                      Ok lar... Joking wif u oni...\n",
            "2      1  Free entry in 2 a wkly comp to win FA Cup fina...\n",
            "3      0  U dun say so early hor... U c already then say...\n",
            "4      0  Nah I don't think he goes to usf, he lives aro...\n",
            "label    5572\n",
            "email    5572\n",
            "dtype: int64\n",
            "\n",
            "Combining datasets...\n",
            "\n",
            "--- Combined Data ---\n",
            "                                               email  label\n",
            "0  From ilug-admin@linux.ie  Tue Sep 24 15:54:23 ...      1\n",
            "1  Return-Path: ler@lerami.lerctr.org\\nDelivery-D...      1\n",
            "2  From ilug-admin@linux.ie  Mon Sep 16 10:44:18 ...      1\n",
            "3  From tammy490t@yahoo.com  Tue Aug 27 05:38:41 ...      1\n",
            "4  From donaldbae@purplehotel.com  Wed Aug 28 11:...      1\n",
            "email    8574\n",
            "label    8574\n",
            "dtype: int64\n",
            "\n",
            "--- Preprocessed Data ---\n",
            "                                               email  label\n",
            "0  from ilug admin linux ie tue sep 24 15 54 23 2...      1\n",
            "1  return path ler lerami lerctr org delivery dat...      1\n",
            "2  from ilug admin linux ie mon sep 16 10 44 18 2...      1\n",
            "3  from tammy490t yahoo com tue aug 27 05 38 41 2...      1\n",
            "4  from donaldbae purplehotel com wed aug 28 11 0...      1\n",
            "email    8574\n",
            "label    8574\n",
            "dtype: int64\n",
            "\n",
            "--- Checking Labels Before Processing ---\n",
            "[1 0]\n",
            "\n",
            "--- Labels After Cleaning ---\n",
            "[1 0]\n",
            "\n",
            "--- Final Label Distribution ---\n",
            "label\n",
            "0    7326\n",
            "1    1248\n",
            "Name: count, dtype: int64\n",
            "\n",
            "--- Model Accuracy: 98.02% ---\n",
            "\n",
            "--- Classification Report ---\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.99      0.99      1449\n",
            "           1       0.93      0.95      0.94       266\n",
            "\n",
            "    accuracy                           0.98      1715\n",
            "   macro avg       0.96      0.97      0.96      1715\n",
            "weighted avg       0.98      0.98      0.98      1715\n",
            "\n",
            "\n",
            "Model and vectorizer saved successfully.\n"
          ]
        }
      ],
      "source": [
        "# Import required libraries\n",
        "import os\n",
        "import tarfile\n",
        "import pandas as pd\n",
        "import re\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "import joblib\n",
        "\n",
        "# Define paths for the dataset\n",
        "SPAM_URL = \"https://spamassassin.apache.org/old/publiccorpus/20030228_spam.tar.bz2\"\n",
        "HAM_URL = \"https://spamassassin.apache.org/old/publiccorpus/20030228_easy_ham.tar.bz2\"\n",
        "DATA_DIR = \"/content/\"\n",
        "# LOCAL_SMS_SPAM_FILE = \"/content/sms_spam.csv\"  # Path to manually uploaded SMS Spam dataset\n",
        "LOCAL_SPAM_CSV_FILE = \"/content/spam.csv\"  # Path to manually uploaded spam.csv\n",
        "\n",
        "# Function to download and extract datasets\n",
        "def download_and_extract(url, extract_to, file_type=\"tar\"):\n",
        "    file_name = os.path.join(extract_to, url.split(\"/\")[-1])\n",
        "    if not os.path.exists(file_name):\n",
        "        print(f\"Downloading {file_name}...\")\n",
        "        os.system(f\"wget -P {extract_to} {url}\")\n",
        "    if file_type == \"tar\":\n",
        "        with tarfile.open(file_name, \"r:*\") as tar:\n",
        "            tar.extractall(path=extract_to)\n",
        "    print(f\"Dataset from {url} downloaded and extracted.\")\n",
        "\n",
        "# Function to load emails from a directory\n",
        "def load_emails_from_directory(directory, label):\n",
        "    emails = []\n",
        "    for file_name in os.listdir(directory):\n",
        "        try:\n",
        "            with open(os.path.join(directory, file_name), 'r', encoding='latin-1') as file:\n",
        "                emails.append({'email': file.read(), 'label': label})\n",
        "        except Exception as e:\n",
        "            print(f\"Error reading file {file_name}: {e}\")\n",
        "    return pd.DataFrame(emails)\n",
        "\n",
        "# Function to load SMS dataset\n",
        "def load_sms_dataset(file_path):\n",
        "    print(f\"Loading SMS Spam dataset from {file_path}...\")\n",
        "    messages = pd.read_csv(file_path, header=0, names=['label', 'email'])\n",
        "    messages['label'] = messages['label'].map({'ham': 0, 'spam': 1})  # Convert labels to binary\n",
        "    print(\"SMS Spam dataset loaded successfully.\")\n",
        "    return messages\n",
        "\n",
        "# Function to load spam.csv dataset\n",
        "def load_spam_csv_dataset(file_path):\n",
        "    print(f\"Loading spam.csv dataset from {file_path}...\")\n",
        "    dataset = pd.read_csv(file_path, encoding=\"latin-1\")\n",
        "    dataset = dataset.rename(columns={\"v1\": \"label\", \"v2\": \"email\"})  # Rename columns\n",
        "    dataset = dataset[[\"label\", \"email\"]]  # Keep only relevant columns\n",
        "    dataset['label'] = dataset['label'].map({'ham': 0, 'spam': 1})  # Map labels to binary\n",
        "    print(\"spam.csv dataset loaded successfully.\")\n",
        "    return dataset\n",
        "\n",
        "# Preprocessing function\n",
        "def preprocess_text(text):\n",
        "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", \"\", text, flags=re.MULTILINE)  # Remove URLs\n",
        "    text = re.sub(r\"\\W\", \" \", text)  # Remove non-alphanumeric characters\n",
        "    text = re.sub(r\"\\s+\", \" \", text)  # Remove extra spaces\n",
        "    return text.lower()  # Convert to lowercase\n",
        "\n",
        "# Load SpamAssassin datasets\n",
        "download_and_extract(SPAM_URL, DATA_DIR, file_type=\"tar\")\n",
        "download_and_extract(HAM_URL, DATA_DIR, file_type=\"tar\")\n",
        "spam_path = os.path.join(DATA_DIR, \"spam\")\n",
        "ham_path = os.path.join(DATA_DIR, \"easy_ham\")\n",
        "spam_data = load_emails_from_directory(spam_path, label=1)\n",
        "ham_data = load_emails_from_directory(ham_path, label=0)\n",
        "\n",
        "# Display individual SpamAssassin datasets\n",
        "print(\"\\n--- SpamAssassin Spam Data ---\")\n",
        "print(spam_data.head())\n",
        "print(spam_data.count())\n",
        "print(\"\\n--- SpamAssassin Ham Data ---\")\n",
        "print(ham_data.head())\n",
        "print(ham_data.count())\n",
        "# # Load the provided SMS Spam dataset\n",
        "# if os.path.exists(LOCAL_SMS_SPAM_FILE):\n",
        "#     sms_data = load_sms_dataset(LOCAL_SMS_SPAM_FILE)\n",
        "# else:\n",
        "#     print(\"SMS Spam dataset not found. Please upload the dataset to the working directory.\")\n",
        "#     sms_data = pd.DataFrame(columns=['label', 'email'])  # Create an empty DataFrame as fallback\n",
        "\n",
        "# Display SMS Spam dataset\n",
        "# print(\"\\n--- SMS Spam Data ---\")\n",
        "# print(sms_data.head())\n",
        "\n",
        "# Load the provided spam.csv dataset\n",
        "if os.path.exists(LOCAL_SPAM_CSV_FILE):\n",
        "    spam_csv_data = load_spam_csv_dataset(LOCAL_SPAM_CSV_FILE)\n",
        "else:\n",
        "    print(\"spam.csv dataset not found. Please upload the dataset to the working directory.\")\n",
        "    spam_csv_data = pd.DataFrame(columns=['label', 'email'])  # Create an empty DataFrame as fallback\n",
        "\n",
        "# Display spam.csv dataset\n",
        "print(\"\\n--- spam.csv Data ---\")\n",
        "print(spam_csv_data.head())\n",
        "print(spam_csv_data.count())\n",
        "\n",
        "# Combine datasets\n",
        "print(\"\\nCombining datasets...\")\n",
        "data = pd.concat([spam_data, ham_data, spam_csv_data], ignore_index=True)\n",
        "\n",
        "# Display combined data\n",
        "print(\"\\n--- Combined Data ---\")\n",
        "print(data.head())\n",
        "print(data.count())\n",
        "\n",
        "# Preprocess emails\n",
        "data['email'] = data['email'].apply(preprocess_text)\n",
        "\n",
        "# Display preprocessed data\n",
        "print(\"\\n--- Preprocessed Data ---\")\n",
        "print(data.head())\n",
        "print(data.count())\n",
        "\n",
        "\n",
        "\n",
        "# Ensure all labels are valid and integers\n",
        "print(\"\\n--- Checking Labels Before Processing ---\")\n",
        "print(data['label'].unique())\n",
        "\n",
        "# Drop rows with invalid or missing labels\n",
        "data = data.dropna(subset=['label'])  # Remove rows with null labels\n",
        "data = data[data['label'].isin([0, 1])]  # Keep only rows with valid binary labels\n",
        "\n",
        "# Convert labels to integers\n",
        "data['label'] = data['label'].astype(int)\n",
        "\n",
        "# Debug: Verify unique labels after cleaning\n",
        "print(\"\\n--- Labels After Cleaning ---\")\n",
        "print(data['label'].unique())\n",
        "\n",
        "# Prepare the dataset\n",
        "X = data['email']\n",
        "y = data['label']\n",
        "\n",
        "# Debug: Ensure y is binary\n",
        "print(\"\\n--- Final Label Distribution ---\")\n",
        "print(y.value_counts())\n",
        "\n",
        "# Preprocess text using TF-IDF\n",
        "vectorizer = TfidfVectorizer(max_features=5000, stop_words='english', strip_accents='unicode', ngram_range=(1, 2))\n",
        "X_tfidf = vectorizer.fit_transform(X)\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_tfidf, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Logistic Regression model\n",
        "model = LogisticRegression(max_iter=1000, class_weight='balanced')\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate the model\n",
        "y_pred = model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"\\n--- Model Accuracy: {accuracy * 100:.2f}% ---\")\n",
        "print(\"\\n--- Classification Report ---\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "# Save the model and vectorizer\n",
        "joblib.dump(model, 'spam_classifier.pkl')\n",
        "joblib.dump(vectorizer, 'tfidf_vectorizer.pkl')\n",
        "print(\"\\nModel and vectorizer saved successfully.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a6JYml2DRhI9",
        "outputId": "a53c44a4-fd1f-483d-9893-5fdaabf15080"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Expanded dataset saved as expanded_dataset.csv.\n"
          ]
        }
      ],
      "source": [
        "# Save the combined dataset as a CSV file\n",
        "expanded_dataset_path = \"expanded_dataset.csv\"\n",
        "data.to_csv(expanded_dataset_path, index=False)\n",
        "print(f\"Expanded dataset saved as {expanded_dataset_path}.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uhH94p3B9AP2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Advanced Bias Mitigation for Combined Dataset\n",
        "import os\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from imblearn.over_sampling import SMOTE\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import Counter\n",
        "\n",
        "# Load dataset\n",
        "file_path = \"/content/expanded_dataset.csv\"\n",
        "if os.path.isfile(file_path):\n",
        "    data = pd.read_csv(file_path)\n",
        "    print(\"Dataset Preview:\")\n",
        "    print(data.head())\n",
        "\n",
        "    # Analyze class distribution\n",
        "    print(\"\\nOriginal Class Distribution:\")\n",
        "    print(data['label'].value_counts())  # Updated to use 'label' column\n",
        "\n",
        "    # Analyze text length\n",
        "    data['text_length'] = data['email'].apply(len)  # Updated to use 'email' column\n",
        "    print(\"\\nText Length Distribution Summary:\")\n",
        "    print(data['text_length'].describe())\n",
        "\n",
        "    # Plot text length distribution\n",
        "    plt.hist(data['text_length'], bins=30, alpha=0.7, color='blue')\n",
        "    plt.title(\"Text Length Distribution\")\n",
        "    plt.xlabel(\"Text Length\")\n",
        "    plt.ylabel(\"Frequency\")\n",
        "    plt.show()\n",
        "\n",
        "    # TF-IDF Analysis\n",
        "    print(\"\\nAnalyzing most important terms in the dataset...\")\n",
        "    vectorizer = TfidfVectorizer(stop_words=\"english\", max_features=10)\n",
        "    tfidf_matrix = vectorizer.fit_transform(data['email'])\n",
        "    important_terms = vectorizer.get_feature_names_out()\n",
        "    print(\"Top 10 Important Terms by TF-IDF:\", important_terms)\n",
        "\n",
        "    # Balance dataset using SMOTE\n",
        "    print(\"\\nApplying SMOTE to balance the dataset...\")\n",
        "    smote = SMOTE(random_state=42)\n",
        "    features = data['email']\n",
        "    labels = data['label']  # Updated to use 'label' column\n",
        "\n",
        "    # Transform text data to numerical form using TF-IDF\n",
        "    vectorizer = TfidfVectorizer(max_features=500)\n",
        "    features_vectorized = vectorizer.fit_transform(features).toarray()\n",
        "\n",
        "    # Apply SMOTE\n",
        "    X_resampled, y_resampled = smote.fit_resample(features_vectorized, labels)\n",
        "\n",
        "    # Check new class distribution\n",
        "    print(\"\\nBalanced Class Distribution After SMOTE:\")\n",
        "    print(Counter(y_resampled))\n",
        "\n",
        "    # Save the balanced dataset\n",
        "    balanced_data = pd.DataFrame(X_resampled, columns=vectorizer.get_feature_names_out())\n",
        "    balanced_data['label'] = y_resampled\n",
        "    balanced_data.to_csv(\"/content/balanced_expanded_dataset.csv\", index=False)\n",
        "    print(\"\\nBalanced dataset saved as 'balanced_expanded_dataset.csv'.\")\n",
        "else:\n",
        "    print(f\"The file '{file_path}' does not exist.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "onQGPwoy9AMc",
        "outputId": "064c9c5e-3fb9-4722-ffa8-99953ab9c32f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset Preview:\n",
            "                                               email  label\n",
            "0  from ilug admin linux ie tue sep 24 15 54 23 2...      1\n",
            "1  return path ler lerami lerctr org delivery dat...      1\n",
            "2  from ilug admin linux ie mon sep 16 10 44 18 2...      1\n",
            "3  from tammy490t yahoo com tue aug 27 05 38 41 2...      1\n",
            "4  from donaldbae purplehotel com wed aug 28 11 0...      1\n",
            "\n",
            "Original Class Distribution:\n",
            "label\n",
            "0    7326\n",
            "1    1248\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Text Length Distribution Summary:\n",
            "count      8574.000000\n",
            "mean       1299.399930\n",
            "std        4995.635785\n",
            "min           1.000000\n",
            "25%          45.000000\n",
            "50%         121.000000\n",
            "75%        1989.000000\n",
            "max      231464.000000\n",
            "Name: text_length, dtype: float64\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Analyzing most important terms in the dataset...\n",
            "Top 10 Important Terms by TF-IDF: ['11' '2002' 'com' 'id' 'list' 'localhost' 'net' 'org' 'received'\n",
            " 'spamassassin']\n",
            "\n",
            "Applying SMOTE to balance the dataset...\n",
            "\n",
            "Balanced Class Distribution After SMOTE:\n",
            "Counter({1: 7326, 0: 7326})\n",
            "\n",
            "Balanced dataset saved as 'balanced_expanded_dataset.csv'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data2 = pd.read_csv(\"/content/balanced_expanded_dataset.csv\")\n",
        "print(\"Expanded Dataset Preview:\")\n",
        "print(data2['label'].value_counts())  # Updated to use 'label' column"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-L0-Sa0p9AJU",
        "outputId": "ea84cd83-1b5f-4020-a50d-b48b9f64400f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Expanded Dataset Preview:\n",
            "label\n",
            "1    7326\n",
            "0    7326\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8_VXmAcv9AG1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RrQ8C0yPRxN_",
        "outputId": "3f2f741f-b580-49c7-ed0d-ab2bdce24d73"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Expanded Dataset Preview:\n",
            "                                               email  label\n",
            "0  from ilug admin linux ie tue sep 24 15 54 23 2...      1\n",
            "1  return path ler lerami lerctr org delivery dat...      1\n",
            "2  from ilug admin linux ie mon sep 16 10 44 18 2...      1\n",
            "3  from tammy490t yahoo com tue aug 27 05 38 41 2...      1\n",
            "4  from donaldbae purplehotel com wed aug 28 11 0...      1\n",
            "Total Emails in Dataset: 8574\n"
          ]
        }
      ],
      "source": [
        "# Verify the saved dataset\n",
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv(\"expanded_dataset.csv\")\n",
        "print(\"Expanded Dataset Preview:\")\n",
        "print(df.head())\n",
        "print(f\"Total Emails in Dataset: {len(df)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gi0cXvCdiJ6v",
        "outputId": "a0acf833-d65c-4256-84a9-918e0185e6a9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy: 97.90%\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      1.00      0.99      1449\n",
            "           1       1.00      0.87      0.93       266\n",
            "\n",
            "    accuracy                           0.98      1715\n",
            "   macro avg       0.99      0.93      0.96      1715\n",
            "weighted avg       0.98      0.98      0.98      1715\n",
            "\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# 3. Train Logistic Regression Model and Evaluate Performance\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Train the Logistic Regression model\n",
        "model = LogisticRegression(max_iter=1000)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model Accuracy: {accuracy * 100:.2f}%\")\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4zURuiPR576-"
      },
      "source": [
        "# **Lets go for deployment**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "U7H8hNAkk80E",
        "outputId": "8c9558b8-7fba-4fd1-fdc8-52cbcdc5508d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: flask-ngrok in /usr/local/lib/python3.10/dist-packages (0.0.25)\n",
            "Requirement already satisfied: Flask>=0.8 in /usr/local/lib/python3.10/dist-packages (from flask-ngrok) (3.0.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from flask-ngrok) (2.32.3)\n",
            "Requirement already satisfied: Werkzeug>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from Flask>=0.8->flask-ngrok) (3.1.3)\n",
            "Requirement already satisfied: Jinja2>=3.1.2 in /usr/local/lib/python3.10/dist-packages (from Flask>=0.8->flask-ngrok) (3.1.4)\n",
            "Requirement already satisfied: itsdangerous>=2.1.2 in /usr/local/lib/python3.10/dist-packages (from Flask>=0.8->flask-ngrok) (2.2.0)\n",
            "Requirement already satisfied: click>=8.1.3 in /usr/local/lib/python3.10/dist-packages (from Flask>=0.8->flask-ngrok) (8.1.7)\n",
            "Requirement already satisfied: blinker>=1.6.2 in /usr/local/lib/python3.10/dist-packages (from Flask>=0.8->flask-ngrok) (1.9.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->flask-ngrok) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->flask-ngrok) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->flask-ngrok) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->flask-ngrok) (2024.8.30)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from Jinja2>=3.1.2->Flask>=0.8->flask-ngrok) (3.0.2)\n",
            "Requirement already satisfied: pyngrok in /usr/local/lib/python3.10/dist-packages (7.2.1)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.10/dist-packages (from pyngrok) (6.0.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install flask-ngrok\n",
        "!pip install pyngrok\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NdQlO7Yc6a77",
        "outputId": "35e78590-c194-44a7-b336-1bdbb4a85752"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "kill: usage: kill [-s sigspec | -n signum | -sigspec] pid | jobspec ... or kill -l [sigspec]\n"
          ]
        }
      ],
      "source": [
        "!kill $(lsof -t -i:5000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "4WdDw_8r9MhW",
        "outputId": "9517f07a-d727-42f0-896d-02b77a39b574"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "nodejs is already the newest version (12.22.9~dfsg-1ubuntu3.6).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 49 not upgraded.\n",
            "\u001b[K\u001b[?25h\n",
            "changed 22 packages, and audited 23 packages in 4s\n",
            "\n",
            "3 packages are looking for funding\n",
            "  run `npm fund` for details\n",
            "\n",
            "1 \u001b[33m\u001b[1mmoderate\u001b[22m\u001b[39m severity vulnerability\n",
            "\n",
            "To address all issues (including breaking changes), run:\n",
            "  npm audit fix --force\n",
            "\n",
            "Run `npm audit` for details.\n",
            "Usage: lt --port [num] <options>\n",
            "\n",
            "Options:\n",
            "  -p, --port                Internal HTTP server port                 [required]\n",
            "  -h, --host                Upstream server providing forwarding\n",
            "                                             [default: \"https://localtunnel.me\"]\n",
            "  -s, --subdomain           Request this subdomain\n",
            "  -l, --local-host          Tunnel traffic to this host instead of localhost,\n",
            "                            override Host header to this host\n",
            "      --local-https         Tunnel traffic to a local HTTPS server     [boolean]\n",
            "      --local-cert          Path to certificate PEM file for local HTTPS server\n",
            "      --local-key           Path to certificate key file for local HTTPS server\n",
            "      --local-ca            Path to certificate authority file for self-signed\n",
            "                            certificates\n",
            "      --allow-invalid-cert  Disable certificate checks for your local HTTPS\n",
            "                            server (ignore cert/key/ca options)        [boolean]\n",
            "  -o, --open                Opens the tunnel URL in your browser\n",
            "      --print-requests      Print basic request info                   [boolean]\n",
            "      --help                Show this help and exit                    [boolean]\n",
            "      --version             Show version number                        [boolean]\n"
          ]
        }
      ],
      "source": [
        "!apt-get install nodejs\n",
        "!npm install -g localtunnel\n",
        "!lt --help\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E23czRP5BBqF"
      },
      "outputs": [],
      "source": [
        "!pkill -f ngrok\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6AusFsgkEwqg"
      },
      "source": [
        "# **Lets apply more advance model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ovCJYnXviSqG",
        "outputId": "3f9aeb26-0485-4741-85d8-453c8ac80e8c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Expanded Dataset Preview:\n",
            "                                               email  label\n",
            "0  from ilug admin linux ie tue sep 24 15 54 23 2...      1\n",
            "1  return path ler lerami lerctr org delivery dat...      1\n",
            "2  from ilug admin linux ie mon sep 16 10 44 18 2...      1\n",
            "3  from tammy490t yahoo com tue aug 27 05 38 41 2...      1\n",
            "4  from donaldbae purplehotel com wed aug 28 11 0...      1\n",
            "\n",
            " before mapping Dataset label:\n",
            "0     1\n",
            "1     1\n",
            "2     1\n",
            "3     1\n",
            "4     1\n",
            "5     1\n",
            "6     1\n",
            "7     1\n",
            "8     1\n",
            "9     1\n",
            "10    1\n",
            "11    1\n",
            "12    1\n",
            "13    1\n",
            "14    1\n",
            "15    1\n",
            "16    1\n",
            "17    1\n",
            "18    1\n",
            "19    1\n",
            "Name: label, dtype: int64 0     from ilug admin linux ie tue sep 24 15 54 23 2...\n",
            "1     return path ler lerami lerctr org delivery dat...\n",
            "2     from ilug admin linux ie mon sep 16 10 44 18 2...\n",
            "3     from tammy490t yahoo com tue aug 27 05 38 41 2...\n",
            "4     from donaldbae purplehotel com wed aug 28 11 0...\n",
            "5     from akadim yahoo com tue dec 3 11 57 06 2002 ...\n",
            "6     from apf wu wien ac at thu sep 19 13 01 55 200...\n",
            "7     from gwfqjulie msn com mon aug 26 21 37 20 200...\n",
            "8     from ilug admin linux ie wed sep 25 10 29 22 2...\n",
            "9     from firstever001 l6 newnamedns com mon sep 2 ...\n",
            "10    from samurislugs1647k84 yahoo com mon sep 2 16...\n",
            "11    from ebayinternetmarketing yahoo com tue dec 3...\n",
            "12    from social admin linux ie tue sep 24 12 53 56...\n",
            "13    from rym insiq us mon sep 2 16 27 20 2002 retu...\n",
            "14    from fholland bigfoot com wed sep 11 19 43 52 ...\n",
            "15    from newpsychic414 mail ru wed aug 28 11 17 19...\n",
            "16    from ra1pohk6x5119 yahoo com mon aug 26 15 14 ...\n",
            "17    from silagra888 hotmail com sun sep 22 21 56 2...\n",
            "18    from lu5guxf4c4149 yahoo com wed aug 28 15 39 ...\n",
            "19    from marie_adolph emailaccount com sat sep 7 2...\n",
            "Name: email, dtype: object\n",
            "\n",
            "Mapped Dataset Preview:\n",
            "   label                                              email\n",
            "0      1  from ilug admin linux ie tue sep 24 15 54 23 2...\n",
            "1      1  return path ler lerami lerctr org delivery dat...\n",
            "2      1  from ilug admin linux ie mon sep 16 10 44 18 2...\n",
            "3      1  from tammy490t yahoo com tue aug 27 05 38 41 2...\n",
            "4      1  from donaldbae purplehotel com wed aug 28 11 0...\n",
            "\n",
            "Logistic Regression Accuracy: 98.02%\n",
            "Random Forest Accuracy: 98.60%\n",
            "Gradient Boosting Accuracy: 97.08%\n",
            "\n",
            "The best model is Random Forest with an accuracy of 98.60%.\n",
            "\n",
            "All models and vectorizer saved successfully.\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "import joblib\n",
        "\n",
        "# Load the dataset\n",
        "data = pd.read_csv(\"expanded_dataset.csv\")\n",
        "\n",
        "# Preview the dataset\n",
        "print(\"Expanded Dataset Preview:\")\n",
        "print(data.head())\n",
        "\n",
        "# Rename columns\n",
        "data.rename(columns={\"v1\": \"label\", \"v2\": \"email\"}, inplace=True)\n",
        "\n",
        "# Drop unnecessary columns\n",
        "data = data[[\"label\", \"email\"]]\n",
        "print(\"\\n before mapping Dataset label:\")\n",
        "print(data[\"label\"].head(20),data[\"email\"].head(20) )\n",
        "# # Map labels to numerical values\n",
        "# data[\"label\"] = data[\"label\"].map({\"ham\": 0, \"spam\": 1})\n",
        "# print(\"\\n Mapped Dataset label:\")\n",
        "# print(data[\"label\"].head(20))\n",
        "\n",
        "# Debug: Ensure the mapping is correct\n",
        "print(\"\\nMapped Dataset Preview:\")\n",
        "print(data.head())\n",
        "\n",
        "# Separate features and labels\n",
        "X = data['email']\n",
        "y = data['label']\n",
        "\n",
        "# Transform text using TF-IDF\n",
        "vectorizer = TfidfVectorizer(max_features=5000, stop_words='english', strip_accents='unicode', ngram_range=(1, 2))\n",
        "X_tfidf = vectorizer.fit_transform(X)\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_tfidf, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize variables to keep track of the best model\n",
        "best_model = None\n",
        "best_accuracy = 0\n",
        "best_model_name = \"\"\n",
        "\n",
        "# Train and save Logistic Regression model\n",
        "logistic_model = LogisticRegression(max_iter=1000, class_weight='balanced')\n",
        "logistic_model.fit(X_train, y_train)\n",
        "y_pred_log = logistic_model.predict(X_test)\n",
        "log_accuracy = accuracy_score(y_test, y_pred_log)\n",
        "print(f\"\\nLogistic Regression Accuracy: {log_accuracy * 100:.2f}%\")\n",
        "joblib.dump(logistic_model, \"logistic_regression.pkl\")\n",
        "\n",
        "if log_accuracy > best_accuracy:\n",
        "    best_accuracy = log_accuracy\n",
        "    best_model = logistic_model\n",
        "    best_model_name = \"Logistic Regression\"\n",
        "\n",
        "# Train and save Random Forest model\n",
        "random_forest_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "random_forest_model.fit(X_train, y_train)\n",
        "y_pred_rf = random_forest_model.predict(X_test)\n",
        "rf_accuracy = accuracy_score(y_test, y_pred_rf)\n",
        "print(f\"Random Forest Accuracy: {rf_accuracy * 100:.2f}%\")\n",
        "joblib.dump(random_forest_model, \"random_forest.pkl\")\n",
        "\n",
        "if rf_accuracy > best_accuracy:\n",
        "    best_accuracy = rf_accuracy\n",
        "    best_model = random_forest_model\n",
        "    best_model_name = \"Random Forest\"\n",
        "\n",
        "# Train and save Gradient Boosting model\n",
        "gradient_boosting_model = GradientBoostingClassifier()\n",
        "gradient_boosting_model.fit(X_train, y_train)\n",
        "y_pred_gb = gradient_boosting_model.predict(X_test)\n",
        "gb_accuracy = accuracy_score(y_test, y_pred_gb)\n",
        "print(f\"Gradient Boosting Accuracy: {gb_accuracy * 100:.2f}%\")\n",
        "joblib.dump(gradient_boosting_model, \"gradient_boosting.pkl\")\n",
        "\n",
        "if gb_accuracy > best_accuracy:\n",
        "    best_accuracy = gb_accuracy\n",
        "    best_model = gradient_boosting_model\n",
        "    best_model_name = \"Gradient Boosting\"\n",
        "\n",
        "# Save the best model\n",
        "joblib.dump(best_model, \"best_spam_classifier.pkl\")\n",
        "print(f\"\\nThe best model is {best_model_name} with an accuracy of {best_accuracy * 100:.2f}%.\")\n",
        "\n",
        "# Save the TF-IDF vectorizer\n",
        "joblib.dump(vectorizer, \"tfidf_vectorizer.pkl\")\n",
        "print(\"\\nAll models and vectorizer saved successfully.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v1grK2tGVdC1"
      },
      "outputs": [],
      "source": [
        "!pkill -f ngrok\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-xoy8v43rcDD",
        "outputId": "c0340bf6-5004-452c-aac2-f141aa81f6ee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.46.2)\n",
            "Collecting datasets\n",
            "  Downloading datasets-3.1.0-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.5.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.26.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.6)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec<=2024.9.0,>=2023.1.0 (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets)\n",
            "  Downloading fsspec-2024.9.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.11.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (0.2.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.17.1)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
            "Downloading datasets-3.1.0-py3-none-any.whl (480 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m480.6/480.6 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2024.9.0-py3-none-any.whl (179 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.3/179.3 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, fsspec, dill, multiprocess, datasets\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2024.10.0\n",
            "    Uninstalling fsspec-2024.10.0:\n",
            "      Successfully uninstalled fsspec-2024.10.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-3.1.0 dill-0.3.8 fsspec-2024.9.0 multiprocess-0.70.16 xxhash-3.5.0\n"
          ]
        }
      ],
      "source": [
        "#Lets apply some more\n",
        "!pip install transformers datasets torch scikit-learn\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 546,
          "referenced_widgets": [
            "b7a441020b7542639d1e95165da580b0",
            "8bfc162d0fcd4adea77a499f50586772"
          ]
        },
        "id": "8CUeFikgrQUE",
        "outputId": "cea322b0-cfc6-4a9c-9a25-e3d0f64c2420"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b7a441020b7542639d1e95165da580b0",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/6859 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8bfc162d0fcd4adea77a499f50586772",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/1715 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n",
            "<ipython-input-81-771210d2c166>:57: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
          ]
        },
        {
          "data": {
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "wandb: Paste an API key from your profile and hit enter, or press ctrl+c to quit:\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.18.7"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20241119_042946-1f0lqguh</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/sakil-sarker-ontario-tech-university/huggingface/runs/1f0lqguh' target=\"_blank\">./results</a></strong> to <a href='https://wandb.ai/sakil-sarker-ontario-tech-university/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/sakil-sarker-ontario-tech-university/huggingface' target=\"_blank\">https://wandb.ai/sakil-sarker-ontario-tech-university/huggingface</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/sakil-sarker-ontario-tech-university/huggingface/runs/1f0lqguh' target=\"_blank\">https://wandb.ai/sakil-sarker-ontario-tech-university/huggingface/runs/1f0lqguh</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='707' max='1287' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [ 707/1287 11:55:51 < 9:48:56, 0.02 it/s, Epoch 1.65/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "      <th>F1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.072300</td>\n",
              "      <td>0.042348</td>\n",
              "      <td>0.986589</td>\n",
              "      <td>0.945055</td>\n",
              "      <td>0.969925</td>\n",
              "      <td>0.957328</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='710' max='1287' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [ 710/1287 11:58:38 < 9:45:39, 0.02 it/s, Epoch 1.65/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "      <th>F1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.072300</td>\n",
              "      <td>0.042348</td>\n",
              "      <td>0.986589</td>\n",
              "      <td>0.945055</td>\n",
              "      <td>0.969925</td>\n",
              "      <td>0.957328</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
        "from datasets import Dataset, DatasetDict\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "import pandas as pd\n",
        "\n",
        "# Load and inspect the dataset\n",
        "data = pd.read_csv(\"expanded_dataset.csv\")\n",
        "print(\"Dataset Columns:\", data.columns)\n",
        "\n",
        "# Ensure columns are properly named\n",
        "if \"v1\" in data.columns and \"v2\" in data.columns:\n",
        "    data.rename(columns={\"v1\": \"label\", \"v2\": \"email\"}, inplace=True)\n",
        "\n",
        "# Map 'ham' and 'spam' to binary labels\n",
        "data[\"label\"] = data[\"label\"].map({\"ham\": 0, \"spam\": 1})\n",
        "\n",
        "# Verify data after preprocessing\n",
        "print(\"Processed Dataset Head:\")\n",
        "print(data.head())\n",
        "\n",
        "# Split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(data[\"email\"], data[\"label\"], test_size=0.2, random_state=42)\n",
        "\n",
        "# Convert to Hugging Face Dataset format\n",
        "train_df = pd.DataFrame({\"text\": X_train, \"label\": y_train})\n",
        "test_df = pd.DataFrame({\"text\": X_test, \"label\": y_test})\n",
        "dataset = DatasetDict({\n",
        "    \"train\": Dataset.from_pandas(train_df),\n",
        "    \"test\": Dataset.from_pandas(test_df),\n",
        "})\n",
        "\n",
        "# Load pre-trained model and tokenizer\n",
        "model_name = \"distilbert-base-uncased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
        "\n",
        "# Tokenize the data\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
        "\n",
        "tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
        "tokenized_dataset = tokenized_dataset.remove_columns([\"text\"])\n",
        "tokenized_dataset = tokenized_dataset.rename_column(\"label\", \"labels\")\n",
        "tokenized_dataset.set_format(\"torch\")\n",
        "\n",
        "# Define metrics\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    predictions = logits.argmax(axis=1)\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(labels, predictions, average=\"binary\")\n",
        "    acc = accuracy_score(labels, predictions)\n",
        "    return {\"accuracy\": acc, \"precision\": precision, \"recall\": recall, \"f1\": f1}\n",
        "\n",
        "# Training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    num_train_epochs=2,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir=\"./logs\",\n",
        "    logging_steps=10,\n",
        "    save_strategy=\"epoch\",\n",
        ")\n",
        "\n",
        "# Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_dataset[\"train\"],\n",
        "    eval_dataset=tokenized_dataset[\"test\"],\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "trainer.train()\n",
        "\n",
        "# Evaluate the model\n",
        "metrics = trainer.evaluate()\n",
        "print(\"Evaluation Metrics:\", metrics)\n",
        "\n",
        "# Save the model and tokenizer\n",
        "model.save_pretrained(\"transformer_spam_model\")\n",
        "tokenizer.save_pretrained(\"transformer_spam_model\")\n",
        "print(\"Model and tokenizer saved successfully.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "AX_KYky1McEd",
        "outputId": "e227d5ae-3ef4-4e72-eba9-c2c959239013"
      },
      "outputs": [
        {
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": [
              "download(\"download_add81925-bd3b-4444-bd45-8016abdb083c\", \"transformer_spam_model.zip\", 247310981)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import shutil\n",
        "\n",
        "# Create a zip file of the model folder\n",
        "shutil.make_archive(\"/content/transformer_spam_model\", 'zip', \"/content/transformer_spam_model\")\n",
        "\n",
        "\n",
        "from google.colab import files\n",
        "\n",
        "# Download the zip file\n",
        "files.download(\"/content/transformer_spam_model.zip\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o74YPOB8RFiD"
      },
      "outputs": [],
      "source": [
        "#Next time load the model and tokenizer\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "\n",
        "# Path to the saved model directory\n",
        "model_path = \"transformer_spam_model\"  # Adjust this if needed\n",
        "\n",
        "# Load the tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "\n",
        "# Load the model\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_path)\n",
        "\n",
        "print(\"Model and tokenizer loaded successfully.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Path to the saved model in Google Drive\n",
        "drive_model_path = \"/content/drive/My Drive/transformer_spam_model\"  # Update this path if needed\n",
        "\n",
        "# Load the tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(drive_model_path)\n",
        "\n",
        "# Load the model\n",
        "model = AutoModelForSequenceClassification.from_pretrained(drive_model_path)\n",
        "\n",
        "print(\"Model and tokenizer loaded successfully.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "suLOtqOSwUuR",
        "outputId": "20045d79-40cb-4673-cd06-88a64247862a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Model and tokenizer loaded successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kAIktoELlM_j",
        "outputId": "800a4952-6ed7-4b4e-b2e0-2e8c7f585ab4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " * Serving Flask app '__main__'\n",
            " * Debug mode: off\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:werkzeug:\u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n",
            " * Running on http://127.0.0.1:8288\n",
            "INFO:werkzeug:\u001b[33mPress CTRL+C to quit\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Access the app at: https://66eb-104-199-136-195.ngrok-free.app\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import random\n",
        "from flask import Flask, request, jsonify, render_template_string\n",
        "import joblib\n",
        "import pandas as pd\n",
        "import threading\n",
        "from pyngrok import ngrok\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Set your ngrok authentication token\n",
        "NGROK_AUTH_TOKEN = \"2oZlSBbJEDOK9wuWQyq4n3PumWA_6wAwkpDus6zrsfAhi4Dcf\"\n",
        "ngrok.set_auth_token(NGROK_AUTH_TOKEN)\n",
        "\n",
        "# Feedback file\n",
        "FEEDBACK_FILE = \"feedback_colab.csv\"\n",
        "\n",
        "# Advanced HTML Template for the UI\n",
        "HTML_TEMPLATE = \"\"\"\n",
        "<!DOCTYPE html>\n",
        "<html lang=\"en\">\n",
        "<head>\n",
        "    <meta charset=\"UTF-8\">\n",
        "    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
        "    <title>Email Spam Detection</title>\n",
        "    <link rel=\"stylesheet\" href=\"https://cdnjs.cloudflare.com/ajax/libs/bootstrap/5.3.0/css/bootstrap.min.css\">\n",
        "    <link href=\"https://fonts.googleapis.com/css2?family=Poppins:wght@300;400;600&display=swap\" rel=\"stylesheet\">\n",
        "    <style>\n",
        "        body {\n",
        "            font-family: 'Poppins', sans-serif;\n",
        "            background: linear-gradient(135deg, #1e3c72, #2a5298);\n",
        "            color: #fff;\n",
        "            min-height: 100vh;\n",
        "            margin: 0;\n",
        "            display: flex;\n",
        "            justify-content: center;\n",
        "            align-items: center;\n",
        "        }\n",
        "        .container {\n",
        "            background-color: rgba(255, 255, 255, 0.1);\n",
        "            border-radius: 15px;\n",
        "            padding: 30px;\n",
        "            max-width: 700px;\n",
        "            box-shadow: 0 4px 15px rgba(0, 0, 0, 0.2);\n",
        "            backdrop-filter: blur(10px);\n",
        "            -webkit-backdrop-filter: blur(10px);\n",
        "        }\n",
        "        h1 {\n",
        "            font-weight: 600;\n",
        "            font-size: 2rem;\n",
        "            text-align: center;\n",
        "        }\n",
        "        textarea {\n",
        "            width: 100%;\n",
        "            height: 150px;\n",
        "            margin-bottom: 20px;\n",
        "            border: none;\n",
        "            border-radius: 10px;\n",
        "            padding: 15px;\n",
        "            background-color: #f8f9fa;\n",
        "            font-size: 1rem;\n",
        "        }\n",
        "        select, button {\n",
        "            width: 100%;\n",
        "            padding: 10px;\n",
        "            font-size: 1rem;\n",
        "            border-radius: 10px;\n",
        "            margin-top: 10px;\n",
        "        }\n",
        "        button {\n",
        "            background: linear-gradient(135deg, #f7971e, #ffd200);\n",
        "            border: none;\n",
        "            color: #333;\n",
        "            font-weight: bold;\n",
        "        }\n",
        "        button:hover {\n",
        "            background: linear-gradient(135deg, #ffd200, #f7971e);\n",
        "            color: #000;\n",
        "        }\n",
        "        .result, .feedback {\n",
        "            margin-top: 20px;\n",
        "            text-align: center;\n",
        "            padding: 10px;\n",
        "            border-radius: 10px;\n",
        "            background: rgba(255, 255, 255, 0.2);\n",
        "        }\n",
        "        .result p {\n",
        "            font-size: 1.2rem;\n",
        "        }\n",
        "        .feedback button {\n",
        "            margin: 5px;\n",
        "            background-color: #007bff;\n",
        "            color: #fff;\n",
        "            border: none;\n",
        "            padding: 10px 15px;\n",
        "            border-radius: 5px;\n",
        "        }\n",
        "        .feedback button:hover {\n",
        "            background-color: #0056b3;\n",
        "        }\n",
        "    </style>\n",
        "</head>\n",
        "<body>\n",
        "    <div class=\"container\">\n",
        "        <h1>Email Spam Detection</h1>\n",
        "        <form id=\"email-form\">\n",
        "            <textarea id=\"email\" placeholder=\"Paste your email content here...\" required></textarea>\n",
        "            <select id=\"model\">\n",
        "                <option value=\"Best Model\">Best Model</option>\n",
        "                <option value=\"Logistic Regression\">Logistic Regression</option>\n",
        "                <option value=\"Random Forest\">Random Forest</option>\n",
        "                <option value=\"Gradient Boosting\">Gradient Boosting</option>\n",
        "            </select>\n",
        "            <button type=\"submit\">Detect</button>\n",
        "        </form>\n",
        "        <div class=\"result\" id=\"result\"></div>\n",
        "        <div class=\"feedback\" id=\"feedback\"></div>\n",
        "    </div>\n",
        "\n",
        "    <script>\n",
        "        document.getElementById('email-form').addEventListener('submit', async (e) => {\n",
        "            e.preventDefault();\n",
        "            const email = document.getElementById('email').value;\n",
        "            const model = document.getElementById('model').value;\n",
        "\n",
        "            document.getElementById('result').innerHTML = \"Processing...\";\n",
        "            document.getElementById('feedback').innerHTML = \"\";\n",
        "\n",
        "            try {\n",
        "                const response = await fetch('/predict', {\n",
        "                    method: 'POST',\n",
        "                    headers: { 'Content-Type': 'application/json' },\n",
        "                    body: JSON.stringify({ email, model }),\n",
        "                });\n",
        "                const data = await response.json();\n",
        "\n",
        "                if (response.ok) {\n",
        "                    document.getElementById('result').innerHTML = `\n",
        "                        <p><strong>Prediction:</strong> ${data.prediction}</p>\n",
        "\n",
        "                        <p>Was this prediction correct?</p>\n",
        "                        <div class=\"d-flex justify-content-center\">\n",
        "                            <button id=\"yes-button\">Yes</button>\n",
        "                            <button id=\"no-button\">No</button>\n",
        "                        </div>\n",
        "                    `;\n",
        "\n",
        "                    document.getElementById('yes-button').addEventListener('click', () => submitFeedback(email, data.prediction, 1));\n",
        "                    document.getElementById('no-button').addEventListener('click', () => submitFeedback(email, data.prediction, 0));\n",
        "                } else {\n",
        "                    document.getElementById('result').innerText = data.error;\n",
        "                }\n",
        "            } catch (err) {\n",
        "                document.getElementById('result').innerText = \"An error occurred. Please try again.\";\n",
        "            }\n",
        "        });\n",
        "\n",
        "        async function submitFeedback(email, predicted, actual) {\n",
        "            try {\n",
        "                const response = await fetch('/feedback', {\n",
        "                    method: 'POST',\n",
        "                    headers: { 'Content-Type': 'application/json' },\n",
        "                    body: JSON.stringify({ email, predicted_label: predicted, actual_label: actual }),\n",
        "                });\n",
        "                const data = await response.json();\n",
        "                document.getElementById('feedback').innerText = data.message;\n",
        "            } catch (err) {\n",
        "                document.getElementById('feedback').innerText = \"An error occurred while submitting feedback.\";\n",
        "            }\n",
        "        }\n",
        "    </script>\n",
        "</body>\n",
        "</html>\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "# Initialize Flask app\n",
        "app = Flask(__name__)\n",
        "\n",
        "@app.route(\"/\")\n",
        "def home():\n",
        "    return render_template_string(HTML_TEMPLATE)\n",
        "\n",
        "@app.route(\"/predict\", methods=[\"POST\"])\n",
        "def predict():\n",
        "    email = request.json.get(\"email\", \"\")\n",
        "    model_name = request.json.get(\"model\", \"Best Model\")\n",
        "\n",
        "    if not email:\n",
        "        return jsonify({\"error\": \"Email content is empty\"}), 400\n",
        "\n",
        "    try:\n",
        "        vectorizer = joblib.load(\"tfidf_vectorizer.pkl\")\n",
        "    except FileNotFoundError:\n",
        "        return jsonify({\"error\": \"TF-IDF vectorizer file not found.\"}), 500\n",
        "\n",
        "    try:\n",
        "        if model_name == \"Logistic Regression\":\n",
        "            model = joblib.load(\"logistic_regression.pkl\")\n",
        "        elif model_name == \"Random Forest\":\n",
        "            model = joblib.load(\"random_forest.pkl\")\n",
        "        elif model_name == \"Gradient Boosting\":\n",
        "            model = joblib.load(\"gradient_boosting.pkl\")\n",
        "        else:\n",
        "            model = joblib.load(\"best_spam_classifier.pkl\")\n",
        "    except FileNotFoundError:\n",
        "        return jsonify({\"error\": f\"Model file for {model_name} not found.\"}), 500\n",
        "\n",
        "    try:\n",
        "        email_vector = vectorizer.transform([email])\n",
        "        prediction = model.predict(email_vector)[0]\n",
        "        result = \"Spam\" if prediction == 1 else \"Ham\"\n",
        "        return jsonify({\"prediction\": result})\n",
        "    except Exception as e:\n",
        "        return jsonify({\"error\": f\"Error during prediction: {str(e)}\"}), 500\n",
        "\n",
        "@app.route(\"/feedback\", methods=[\"POST\"])\n",
        "def feedback():\n",
        "    email = request.json.get(\"email\", \"\")\n",
        "    predicted_label = request.json.get(\"predicted_label\", -1)\n",
        "    actual_label = request.json.get(\"actual_label\", -1)\n",
        "\n",
        "    if not email or predicted_label == -1 or actual_label == -1:\n",
        "        return jsonify({\"error\": \"Invalid feedback data\"}), 400\n",
        "\n",
        "    feedback_data = pd.DataFrame([{\n",
        "        \"email\": email,\n",
        "        \"predicted_label\": predicted_label,\n",
        "        \"actual_label\": actual_label\n",
        "    }])\n",
        "    feedback_data.to_csv(FEEDBACK_FILE, mode=\"a\", header=not os.path.exists(FEEDBACK_FILE), index=False)\n",
        "\n",
        "    feedback_count = len(pd.read_csv(FEEDBACK_FILE))\n",
        "    if feedback_count >= 100:\n",
        "        retrain_model()\n",
        "\n",
        "    return jsonify({\"message\": \"Feedback submitted successfully\"})\n",
        "\n",
        "def retrain_model():\n",
        "    print(\"\\nRetraining the model with feedback data...\")\n",
        "    feedback_data = pd.read_csv(FEEDBACK_FILE)\n",
        "    X = feedback_data['email']\n",
        "    y = feedback_data['actual_label']\n",
        "\n",
        "    vectorizer = TfidfVectorizer(max_features=5000, stop_words=\"english\", ngram_range=(1, 2))\n",
        "    X_tfidf = vectorizer.fit_transform(X)\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X_tfidf, y, test_size=0.2, random_state=42)\n",
        "    model = LogisticRegression(max_iter=1000, class_weight='balanced')\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    joblib.dump(model, \"best_spam_classifier.pkl\")\n",
        "    print(\"\\nModel retrained and updated successfully.\")\n",
        "\n",
        "def start_server_and_tunnel():\n",
        "    dynamic_port = random.randint(8000, 9000)\n",
        "    threading.Thread(target=lambda: app.run(port=dynamic_port)).start()\n",
        "    public_url = ngrok.connect(dynamic_port).public_url\n",
        "    print(f\"Access the app at: {public_url}\")\n",
        "\n",
        "start_server_and_tunnel()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "7uj5_E3tlQw6"
      },
      "outputs": [],
      "source": [
        "!pkill -f ngrok"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BRVvVE8fQHcd",
        "outputId": "c7656ae2-76ac-4672-ba93-4bf88568398d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Model and tokenizer loaded successfully.\n",
            " * Serving Flask app '__main__'\n",
            " * Debug mode: off\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:werkzeug:\u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n",
            " * Running on http://127.0.0.1:8807\n",
            "INFO:werkzeug:\u001b[33mPress CTRL+C to quit\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Access the app at: https://6629-104-199-136-195.ngrok-free.app\n"
          ]
        }
      ],
      "source": [
        "#Advance UI\n",
        "\n",
        "import os\n",
        "import random\n",
        "from flask import Flask, request, jsonify, render_template_string\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "import pandas as pd\n",
        "import threading\n",
        "from pyngrok import ngrok\n",
        "import joblib\n",
        "\n",
        "# Set your ngrok authentication token\n",
        "NGROK_AUTH_TOKEN = \"2oZlSBbJEDOK9wuWQyq4n3PumWA_6wAwkpDus6zrsfAhi4Dcf\"\n",
        "ngrok.set_auth_token(NGROK_AUTH_TOKEN)\n",
        "\n",
        "# Feedback file\n",
        "FEEDBACK_FILE = \"feedback_colab.csv\"\n",
        "\n",
        "# Enhanced HTML Template for the UI\n",
        "HTML_TEMPLATE = \"\"\"\n",
        "<!DOCTYPE html>\n",
        "<html lang=\"en\">\n",
        "<head>\n",
        "    <meta charset=\"UTF-8\">\n",
        "    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
        "    <title>Email Spam Detection</title>\n",
        "    <link rel=\"stylesheet\" href=\"https://cdnjs.cloudflare.com/ajax/libs/bootstrap/5.3.0/css/bootstrap.min.css\">\n",
        "    <link href=\"https://fonts.googleapis.com/css2?family=Poppins:wght@300;400;600&display=swap\" rel=\"stylesheet\">\n",
        "    <style>\n",
        "        body {\n",
        "            font-family: 'Poppins', sans-serif;\n",
        "            background: linear-gradient(135deg, #1e3c72, #2a5298);\n",
        "            color: #fff;\n",
        "            min-height: 100vh;\n",
        "            margin: 0;\n",
        "            display: flex;\n",
        "            justify-content: center;\n",
        "            align-items: center;\n",
        "        }\n",
        "        .container {\n",
        "            background-color: rgba(255, 255, 255, 0.1);\n",
        "            border-radius: 15px;\n",
        "            padding: 30px;\n",
        "            max-width: 700px;\n",
        "            box-shadow: 0 4px 15px rgba(0, 0, 0, 0.2);\n",
        "            backdrop-filter: blur(10px);\n",
        "            -webkit-backdrop-filter: blur(10px);\n",
        "        }\n",
        "        h1 {\n",
        "            font-weight: 600;\n",
        "            font-size: 2rem;\n",
        "            text-align: center;\n",
        "        }\n",
        "        textarea {\n",
        "            width: 100%;\n",
        "            height: 150px;\n",
        "            margin-bottom: 20px;\n",
        "            border: none;\n",
        "            border-radius: 10px;\n",
        "            padding: 15px;\n",
        "            background-color: #f8f9fa;\n",
        "            font-size: 1rem;\n",
        "        }\n",
        "        select, button {\n",
        "            width: 100%;\n",
        "            padding: 10px;\n",
        "            font-size: 1rem;\n",
        "            border-radius: 10px;\n",
        "            margin-top: 10px;\n",
        "        }\n",
        "        button {\n",
        "            background: linear-gradient(135deg, #f7971e, #ffd200);\n",
        "            border: none;\n",
        "            color: #333;\n",
        "            font-weight: bold;\n",
        "        }\n",
        "        button:hover {\n",
        "            background: linear-gradient(135deg, #ffd200, #f7971e);\n",
        "            color: #000;\n",
        "        }\n",
        "        .result, .feedback {\n",
        "            margin-top: 20px;\n",
        "            text-align: center;\n",
        "            padding: 10px;\n",
        "            border-radius: 10px;\n",
        "            background: rgba(255, 255, 255, 0.2);\n",
        "        }\n",
        "        .result p {\n",
        "            font-size: 1.2rem;\n",
        "        }\n",
        "        .feedback button {\n",
        "            margin: 5px;\n",
        "            background-color: #007bff;\n",
        "            color: #fff;\n",
        "            border: none;\n",
        "            padding: 10px 15px;\n",
        "            border-radius: 5px;\n",
        "        }\n",
        "        .feedback button:hover {\n",
        "            background-color: #0056b3;\n",
        "        }\n",
        "    </style>\n",
        "</head>\n",
        "<body>\n",
        "    <div class=\"container\">\n",
        "        <h1>Email Spam Detection</h1>\n",
        "        <form id=\"email-form\">\n",
        "            <textarea id=\"email\" placeholder=\"Paste your email content here...\" required></textarea>\n",
        "            <select id=\"model\">\n",
        "                <option value=\"Transformer\">Transformer (DistilBERT)</option>\n",
        "                <option value=\"Logistic Regression\">Logistic Regression</option>\n",
        "                <option value=\"Random Forest\">Random Forest</option>\n",
        "                <option value=\"Gradient Boosting\">Gradient Boosting</option>\n",
        "            </select>\n",
        "            <button type=\"submit\">Detect</button>\n",
        "        </form>\n",
        "        <div class=\"result\" id=\"result\"></div>\n",
        "        <div class=\"feedback\" id=\"feedback\"></div>\n",
        "    </div>\n",
        "\n",
        "    <script>\n",
        "        document.getElementById('email-form').addEventListener('submit', async (e) => {\n",
        "            e.preventDefault();\n",
        "            const email = document.getElementById('email').value;\n",
        "            const model = document.getElementById('model').value;\n",
        "\n",
        "            document.getElementById('result').innerHTML = \"Processing...\";\n",
        "            document.getElementById('feedback').innerHTML = \"\";\n",
        "\n",
        "            try {\n",
        "                const response = await fetch('/predict', {\n",
        "                    method: 'POST',\n",
        "                    headers: { 'Content-Type': 'application/json' },\n",
        "                    body: JSON.stringify({ email, model }),\n",
        "                });\n",
        "                const data = await response.json();\n",
        "\n",
        "                if (response.ok) {\n",
        "                    document.getElementById('result').innerHTML = `\n",
        "                        <p><strong>Prediction:</strong> ${data.prediction}</p>\n",
        "                        <p>Was this prediction correct?</p>\n",
        "                        <div class=\"d-flex justify-content-center\">\n",
        "                            <button id=\"yes-button\">Yes</button>\n",
        "                            <button id=\"no-button\">No</button>\n",
        "                        </div>\n",
        "                    `;\n",
        "\n",
        "                    document.getElementById('yes-button').addEventListener('click', () => submitFeedback(email, data.prediction, 1));\n",
        "                    document.getElementById('no-button').addEventListener('click', () => submitFeedback(email, data.prediction, 0));\n",
        "                } else {\n",
        "                    document.getElementById('result').innerText = data.error;\n",
        "                }\n",
        "            } catch (err) {\n",
        "                document.getElementById('result').innerText = \"An error occurred. Please try again.\";\n",
        "            }\n",
        "        });\n",
        "\n",
        "        async function submitFeedback(email, predicted, actual) {\n",
        "            try {\n",
        "                const response = await fetch('/feedback', {\n",
        "                    method: 'POST',\n",
        "                    headers: { 'Content-Type': 'application/json' },\n",
        "                    body: JSON.stringify({ email, predicted_label: predicted, actual_label: actual }),\n",
        "                });\n",
        "                const data = await response.json();\n",
        "                document.getElementById('feedback').innerText = data.message;\n",
        "            } catch (err) {\n",
        "                document.getElementById('feedback').innerText = \"An error occurred while submitting feedback.\";\n",
        "            }\n",
        "        }\n",
        "    </script>\n",
        "</body>\n",
        "</html>\n",
        "\"\"\"\n",
        "\n",
        "# Initialize Flask app\n",
        "app = Flask(__name__)\n",
        "\n",
        "# Load the Hugging Face Transformer model and tokenizer\n",
        "# transformer_model_path = \"/content/transformer_spam_model\"  # Update path if needed\n",
        "# tokenizer = AutoTokenizer.from_pretrained(transformer_model_path)\n",
        "# transformer_model = AutoModelForSequenceClassification.from_pretrained(transformer_model_path)\n",
        "\n",
        "from google.colab import drive\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Path to the saved model in Google Drive\n",
        "drive_model_path = \"/content/drive/My Drive/transformer_spam_model\"  # Update this path if needed\n",
        "\n",
        "# Load the tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(drive_model_path)\n",
        "\n",
        "# Load the model\n",
        "transformer_model = AutoModelForSequenceClassification.from_pretrained(drive_model_path)\n",
        "\n",
        "print(\"Model and tokenizer loaded successfully.\")\n",
        "\n",
        "# tokenizer = AutoTokenizer.from_pretrained(transformer_model_path)\n",
        "# transformer_model = AutoModelForSequenceClassification.from_pretrained(transformer_model_path)\n",
        "\n",
        "@app.route(\"/\")\n",
        "def home():\n",
        "    return render_template_string(HTML_TEMPLATE)\n",
        "\n",
        "@app.route(\"/predict\", methods=[\"POST\"])\n",
        "def predict():\n",
        "    email = request.json.get(\"email\", \"\")\n",
        "    model_name = request.json.get(\"model\", \"Transformer\")\n",
        "\n",
        "    if not email:\n",
        "        return jsonify({\"error\": \"Email content is empty\"}), 400\n",
        "\n",
        "    try:\n",
        "        if model_name == \"Transformer\":\n",
        "            inputs = tokenizer(email, return_tensors=\"pt\", truncation=True, padding=True)\n",
        "            outputs = transformer_model(**inputs)\n",
        "            prediction = outputs.logits.argmax(-1).item()\n",
        "            result = \"Spam\" if prediction == 1 else \"Ham\"\n",
        "        elif model_name == \"Logistic Regression\":\n",
        "            vectorizer = joblib.load(\"tfidf_vectorizer.pkl\")\n",
        "            model = joblib.load(\"logistic_regression.pkl\")\n",
        "            email_vector = vectorizer.transform([email])\n",
        "            prediction = model.predict(email_vector)[0]\n",
        "            result = \"Spam\" if prediction == 1 else \"Ham\"\n",
        "        elif model_name == \"Random Forest\":\n",
        "            vectorizer = joblib.load(\"tfidf_vectorizer.pkl\")\n",
        "            model = joblib.load(\"random_forest.pkl\")\n",
        "            email_vector = vectorizer.transform([email])\n",
        "            prediction = model.predict(email_vector)[0]\n",
        "            result = \"Spam\" if prediction == 1 else \"Ham\"\n",
        "        elif model_name == \"Gradient Boosting\":\n",
        "            vectorizer = joblib.load(\"tfidf_vectorizer.pkl\")\n",
        "            model = joblib.load(\"gradient_boosting.pkl\")\n",
        "            email_vector = vectorizer.transform([email])\n",
        "            prediction = model.predict(email_vector)[0]\n",
        "            result = \"Spam\" if prediction == 1 else \"Ham\"\n",
        "        else:\n",
        "            return jsonify({\"error\": \"Model not recognized\"}), 400\n",
        "\n",
        "        return jsonify({\"prediction\": result})\n",
        "    except Exception as e:\n",
        "        return jsonify({\"error\": f\"Error during prediction: {str(e)}\"}), 500\n",
        "\n",
        "@app.route(\"/feedback\", methods=[\"POST\"])\n",
        "def feedback():\n",
        "    try:\n",
        "        email = request.json.get(\"email\", \"\")\n",
        "        predicted_label = request.json.get(\"predicted_label\", -1)\n",
        "        actual_label = request.json.get(\"actual_label\", -1)\n",
        "\n",
        "        # Validate feedback data\n",
        "        if not email or predicted_label == -1 or actual_label == -1:\n",
        "            return jsonify({\"error\": \"Invalid feedback data\"}), 400\n",
        "\n",
        "        # Log feedback for debugging\n",
        "        print(\"Received Feedback Data:\")\n",
        "        print(f\"Email: {email}\")\n",
        "        print(f\"Predicted Label: {predicted_label}\")\n",
        "        print(f\"Actual Label: {actual_label}\")\n",
        "\n",
        "        # Save feedback to CSV\n",
        "        feedback_data = pd.DataFrame([{\n",
        "            \"email\": email,\n",
        "            \"predicted_label\": predicted_label,\n",
        "            \"actual_label\": actual_label\n",
        "        }])\n",
        "        feedback_data.to_csv(FEEDBACK_FILE, mode=\"a\", header=not os.path.exists(FEEDBACK_FILE), index=False)\n",
        "\n",
        "        # Check feedback count and retrain if needed\n",
        "        feedback_count = len(pd.read_csv(FEEDBACK_FILE))\n",
        "        print(f\"Feedback count: {feedback_count}\")\n",
        "        if feedback_count >= 100:\n",
        "            retrain_model()\n",
        "\n",
        "        return jsonify({\"message\": \"Feedback submitted successfully\"})\n",
        "    except Exception as e:\n",
        "        print(f\"Error in feedback endpoint: {str(e)}\")\n",
        "        return jsonify({\"error\": f\"Error during feedback processing: {str(e)}\"}), 500\n",
        "\n",
        "def retrain_model():\n",
        "    print(\"\\nRetraining the model with feedback data...\")\n",
        "    feedback_data = pd.read_csv(FEEDBACK_FILE)\n",
        "    X = feedback_data['email']\n",
        "    y = feedback_data['actual_label']\n",
        "\n",
        "    # Perform retraining for one of the models (e.g., Logistic Regression)\n",
        "    vectorizer = TfidfVectorizer(max_features=5000, stop_words=\"english\", ngram_range=(1, 2))\n",
        "    X_tfidf = vectorizer.fit_transform(X)\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X_tfidf, y, test_size=0.2, random_state=42)\n",
        "    model = LogisticRegression(max_iter=1000, class_weight='balanced')\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    # Save the retrained model\n",
        "    joblib.dump(model, \"best_spam_classifier.pkl\")\n",
        "    print(\"\\nModel retrained and updated successfully.\")\n",
        "\n",
        "def start_server_and_tunnel():\n",
        "    dynamic_port = random.randint(8000, 9000)\n",
        "    threading.Thread(target=lambda: app.run(port=dynamic_port)).start()\n",
        "    public_url = ngrok.connect(dynamic_port).public_url\n",
        "    print(f\"Access the app at: {public_url}\")\n",
        "\n",
        "# Start the server\n",
        "start_server_and_tunnel()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZiUwn4LDQHZO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bc233688-aa41-45ee-e465-36a7613b8f81"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: markdown in /usr/local/lib/python3.10/dist-packages (3.7)\n"
          ]
        }
      ],
      "source": [
        "!pip install markdown\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z4WjlHth7r7B",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0edb1a1b-1158-4c0e-a824-04b6400a4f43"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Model and tokenizer loaded successfully.\n",
            " * Serving Flask app '__main__'\n",
            " * Debug mode: off\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:werkzeug:\u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n",
            " * Running on http://127.0.0.1:8039\n",
            "INFO:werkzeug:\u001b[33mPress CTRL+C to quit\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Access the app at: https://1b7d-104-199-136-195.ngrok-free.app\n"
          ]
        }
      ],
      "source": [
        "#Advance UI\n",
        "\n",
        "import os\n",
        "import random\n",
        "from flask import Flask, request, jsonify, render_template_string\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "import pandas as pd\n",
        "import threading\n",
        "from pyngrok import ngrok\n",
        "import joblib\n",
        "\n",
        "# Set your ngrok authentication token\n",
        "NGROK_AUTH_TOKEN = \"2oZlSBbJEDOK9wuWQyq4n3PumWA_6wAwkpDus6zrsfAhi4Dcf\"\n",
        "ngrok.set_auth_token(NGROK_AUTH_TOKEN)\n",
        "\n",
        "# Feedback file\n",
        "FEEDBACK_FILE = \"feedback_colab.csv\"\n",
        "\n",
        "# Enhanced HTML Template for the UI\n",
        "HTML_TEMPLATE = \"\"\"\n",
        "<!DOCTYPE html>\n",
        "<html lang=\"en\">\n",
        "<head>\n",
        "    <meta charset=\"UTF-8\">\n",
        "    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
        "    <title>Email Spam Detection</title>\n",
        "    <link rel=\"stylesheet\" href=\"https://cdnjs.cloudflare.com/ajax/libs/bootstrap/5.3.0/css/bootstrap.min.css\">\n",
        "    <link href=\"https://fonts.googleapis.com/css2?family=Poppins:wght@300;400;600&display=swap\" rel=\"stylesheet\">\n",
        "    <style>\n",
        "        body {\n",
        "            font-family: 'Poppins', sans-serif;\n",
        "            background: linear-gradient(135deg, #1e3c72, #2a5298);\n",
        "            color: #fff;\n",
        "            min-height: 100vh;\n",
        "            margin: 0;\n",
        "            display: flex;\n",
        "            justify-content: center;\n",
        "            align-items: center;\n",
        "            padding: 20px;\n",
        "        }\n",
        "        .container {\n",
        "            background-color: rgba(255, 255, 255, 0.15);\n",
        "            border-radius: 15px;\n",
        "            padding: 30px;\n",
        "            max-width: 800px;\n",
        "            width: 100%;\n",
        "            box-shadow: 0 4px 15px rgba(0, 0, 0, 0.2);\n",
        "            backdrop-filter: blur(10px);\n",
        "            -webkit-backdrop-filter: blur(10px);\n",
        "        }\n",
        "        h1 {\n",
        "            font-weight: 700;\n",
        "            font-size: 2.5rem;\n",
        "            text-align: center;\n",
        "        }\n",
        "        textarea {\n",
        "            width: 100%;\n",
        "            height: 150px;\n",
        "            margin-bottom: 20px;\n",
        "            border: none;\n",
        "            border-radius: 10px;\n",
        "            padding: 15px;\n",
        "            background-color: #f8f9fa;\n",
        "            font-size: 1rem;\n",
        "            resize: none;\n",
        "        }\n",
        "        select, button {\n",
        "            width: 100%;\n",
        "            padding: 12px;\n",
        "            font-size: 1rem;\n",
        "            border-radius: 10px;\n",
        "            margin-top: 10px;\n",
        "            border: none;\n",
        "        }\n",
        "        button {\n",
        "            background: linear-gradient(135deg, #ff7e5f, #feb47b);\n",
        "            color: #fff;\n",
        "            font-weight: bold;\n",
        "            transition: all 0.3s ease-in-out;\n",
        "        }\n",
        "        button:hover {\n",
        "            background: linear-gradient(135deg, #feb47b, #ff7e5f);\n",
        "            transform: scale(1.05);\n",
        "        }\n",
        "        .result, .feedback {\n",
        "            margin-top: 20px;\n",
        "            text-align: center;\n",
        "            padding: 15px;\n",
        "            border-radius: 10px;\n",
        "            background: rgba(255, 255, 255, 0.2);\n",
        "        }\n",
        "        .result p, .feedback p {\n",
        "            font-size: 1.2rem;\n",
        "        }\n",
        "        .feedback button {\n",
        "            margin: 5px;\n",
        "            background-color: #007bff;\n",
        "            color: #fff;\n",
        "            padding: 10px 15px;\n",
        "            border-radius: 5px;\n",
        "            border: none;\n",
        "            transition: all 0.3s ease-in-out;\n",
        "        }\n",
        "        .feedback button:hover {\n",
        "            background-color: #0056b3;\n",
        "            transform: scale(1.1);\n",
        "        }\n",
        "        #confidence-chart, #metrics-chart {\n",
        "            margin-top: 20px;\n",
        "        }\n",
        "        @media (max-width: 768px) {\n",
        "            h1 {\n",
        "                font-size: 2rem;\n",
        "            }\n",
        "            textarea {\n",
        "                height: 120px;\n",
        "            }\n",
        "            .container {\n",
        "                padding: 20px;\n",
        "            }\n",
        "        }\n",
        "    </style>\n",
        "</head>\n",
        "<body>\n",
        "    <div class=\"container\">\n",
        "        <h1>Email Spam Detection</h1>\n",
        "        <form id=\"email-form\">\n",
        "            <textarea id=\"email\" placeholder=\"Paste your email content here...\" required></textarea>\n",
        "            <select id=\"model\">\n",
        "                <option value=\"Transformer\">Transformer (DistilBERT)</option>\n",
        "                <option value=\"Logistic Regression\">Logistic Regression</option>\n",
        "                <option value=\"Random Forest\">Random Forest</option>\n",
        "                <option value=\"Gradient Boosting\">Gradient Boosting</option>\n",
        "            </select>\n",
        "            <button type=\"submit\">Detect</button>\n",
        "        </form>\n",
        "        <div id=\"result-container\">\n",
        "            <div class=\"result\" id=\"result\"></div>\n",
        "            <canvas id=\"confidence-chart\" style=\"max-width: 100%; margin: 20px auto;\"></canvas>\n",
        "            <div id=\"performance-metrics\">\n",
        "                <h3 class=\"text-center\">Confidence Metrics</h3>\n",
        "                <canvas id=\"metrics-chart\" style=\"max-width: 100%; margin: 0 auto;\"></canvas>\n",
        "            </div>\n",
        "        </div>\n",
        "        <div class=\"feedback\" id=\"feedback\"></div>\n",
        "    </div>\n",
        "    <script src=\"https://cdn.jsdelivr.net/npm/chart.js\"></script>\n",
        "    <script>\n",
        "        document.getElementById('email-form').addEventListener('submit', async (e) => {\n",
        "            e.preventDefault();\n",
        "            const email = document.getElementById('email').value;\n",
        "            const model = document.getElementById('model').value;\n",
        "\n",
        "            document.getElementById('result').innerHTML = \"Processing...\";\n",
        "            document.getElementById('feedback').innerHTML = \"\";\n",
        "\n",
        "            try {\n",
        "                const response = await fetch('/predict', {\n",
        "                    method: 'POST',\n",
        "                    headers: { 'Content-Type': 'application/json' },\n",
        "                    body: JSON.stringify({ email, model }),\n",
        "                });\n",
        "                const data = await response.json();\n",
        "\n",
        "                if (response.ok) {\n",
        "                    document.getElementById('result').innerHTML = `\n",
        "                        <p><strong>Prediction:</strong> ${data.prediction}</p>\n",
        "                        <p>Was this prediction correct?</p>\n",
        "                        <div class=\"d-flex justify-content-center\">\n",
        "                            <button id=\"yes-button\" class=\"feedback-button\">Yes</button>\n",
        "                            <button id=\"no-button\" class=\"feedback-button\">No</button>\n",
        "                        </div>\n",
        "                    `;\n",
        "                    const confidenceCtx = document.getElementById('confidence-chart').getContext('2d');\n",
        "                    new Chart(confidenceCtx, {\n",
        "                        type: 'bar',\n",
        "                        data: {\n",
        "                            labels: ['Spam', 'Ham'],\n",
        "                            datasets: [{\n",
        "                                label: 'Confidence Score (%)',\n",
        "                                data: [data.confidence[1] * 100, data.confidence[0] * 100],\n",
        "                                backgroundColor: ['#ff6384', '#36a2eb'],\n",
        "                            }]\n",
        "                        },\n",
        "                        options: { responsive: true },\n",
        "                    });\n",
        "\n",
        "                    document.getElementById('yes-button').addEventListener('click', () => submitFeedback(email, data.prediction, 1));\n",
        "                    document.getElementById('no-button').addEventListener('click', () => submitFeedback(email, data.prediction, 0));\n",
        "                } else {\n",
        "                    document.getElementById('result').innerText = data.error || \"An error occurred. Please try again.\";\n",
        "                }\n",
        "            } catch (err) {\n",
        "                document.getElementById('result').innerText = \"An error occurred. Please try again.\";\n",
        "            }\n",
        "        });\n",
        "\n",
        "        async function submitFeedback(email, predicted, actual) {\n",
        "            try {\n",
        "                const response = await fetch('/feedback', {\n",
        "                    method: 'POST',\n",
        "                    headers: { 'Content-Type': 'application/json' },\n",
        "                    body: JSON.stringify({ email, predicted_label: predicted, actual_label: actual }),\n",
        "                });\n",
        "                const data = await response.json();\n",
        "                document.getElementById('feedback').innerText = data.message;\n",
        "            } catch (err) {\n",
        "                document.getElementById('feedback').innerText = \"An error occurred while submitting feedback.\";\n",
        "            }\n",
        "        }\n",
        "    </script>\n",
        "</body>\n",
        "</html>\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "# Initialize Flask app\n",
        "app = Flask(__name__)\n",
        "\n",
        "# Load the Hugging Face Transformer model and tokenizer\n",
        "# transformer_model_path = \"/content/transformer_spam_model\"  # Update path if needed\n",
        "# tokenizer = AutoTokenizer.from_pretrained(transformer_model_path)\n",
        "# transformer_model = AutoModelForSequenceClassification.from_pretrained(transformer_model_path)\n",
        "\n",
        "from google.colab import drive\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Path to the saved model in Google Drive\n",
        "drive_model_path = \"/content/drive/My Drive/transformer_spam_model\"  # Update this path if needed\n",
        "\n",
        "# Load the tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(drive_model_path)\n",
        "\n",
        "# Load the model\n",
        "transformer_model = AutoModelForSequenceClassification.from_pretrained(drive_model_path)\n",
        "\n",
        "print(\"Model and tokenizer loaded successfully.\")\n",
        "\n",
        "# tokenizer = AutoTokenizer.from_pretrained(transformer_model_path)\n",
        "# transformer_model = AutoModelForSequenceClassification.from_pretrained(transformer_model_path)\n",
        "\n",
        "@app.route(\"/\")\n",
        "def home():\n",
        "    return render_template_string(HTML_TEMPLATE)\n",
        "\n",
        "@app.route(\"/predict\", methods=[\"POST\"])\n",
        "def predict():\n",
        "    email = request.json.get(\"email\", \"\")\n",
        "    model_name = request.json.get(\"model\", \"Transformer\")\n",
        "\n",
        "    if not email:\n",
        "        return jsonify({\"error\": \"Email content is empty\"}), 400\n",
        "\n",
        "    try:\n",
        "        # Load vectorizer and models\n",
        "        vectorizer_path = \"/content/tfidf_vectorizer.pkl\"\n",
        "        logistic_model_path = \"/content/logistic_regression.pkl\"\n",
        "        random_forest_model_path = \"/content/random_forest.pkl\"\n",
        "        gradient_boosting_model_path = \"/content/gradient_boosting.pkl\"\n",
        "\n",
        "        vectorizer = joblib.load(vectorizer_path)\n",
        "\n",
        "        if model_name == \"Transformer\":\n",
        "            # Transformer model prediction\n",
        "            inputs = tokenizer(email, return_tensors=\"pt\", truncation=True, padding=True)\n",
        "            outputs = transformer_model(**inputs)\n",
        "            logits = outputs.logits\n",
        "            probabilities = logits.softmax(dim=-1).tolist()[0]\n",
        "            prediction = int(np.argmax(probabilities))\n",
        "            result = \"Spam\" if prediction == 1 else \"Ham\"\n",
        "\n",
        "        elif model_name == \"Logistic Regression\":\n",
        "            # Logistic Regression model prediction\n",
        "            logistic_model = joblib.load(logistic_model_path)\n",
        "            email_vector = vectorizer.transform([email])\n",
        "            probabilities = logistic_model.predict_proba(email_vector)[0]\n",
        "            prediction = int(np.argmax(probabilities))\n",
        "            result = \"Spam\" if prediction == 1 else \"Ham\"\n",
        "\n",
        "        elif model_name == \"Random Forest\":\n",
        "            # Random Forest model prediction\n",
        "            random_forest_model = joblib.load(random_forest_model_path)\n",
        "            email_vector = vectorizer.transform([email])\n",
        "            probabilities = random_forest_model.predict_proba(email_vector)[0]\n",
        "            prediction = int(np.argmax(probabilities))\n",
        "            result = \"Spam\" if prediction == 1 else \"Ham\"\n",
        "\n",
        "        elif model_name == \"Gradient Boosting\":\n",
        "            # Gradient Boosting model prediction\n",
        "            gradient_boosting_model = joblib.load(gradient_boosting_model_path)\n",
        "            email_vector = vectorizer.transform([email])\n",
        "            probabilities = gradient_boosting_model.predict_proba(email_vector)[0]\n",
        "            prediction = int(np.argmax(probabilities))\n",
        "            result = \"Spam\" if prediction == 1 else \"Ham\"\n",
        "\n",
        "        else:\n",
        "            return jsonify({\"error\": \"Model not recognized\"}), 400\n",
        "\n",
        "        # Return the prediction and confidence scores\n",
        "        return jsonify({\n",
        "            \"prediction\": result,\n",
        "            \"confidence\": [round(probabilities[0] * 100, 2), round(probabilities[1] * 100, 2)],\n",
        "        })\n",
        "\n",
        "    except FileNotFoundError as e:\n",
        "        return jsonify({\"error\": f\"File not found: {str(e)}\"}), 500\n",
        "    except Exception as e:\n",
        "        return jsonify({\"error\": f\"Error during prediction: {str(e)}\"}), 500\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "@app.route(\"/feedback\", methods=[\"POST\"])\n",
        "def feedback():\n",
        "    try:\n",
        "        email = request.json.get(\"email\", \"\")\n",
        "        predicted_label = request.json.get(\"predicted_label\", -1)\n",
        "        actual_label = request.json.get(\"actual_label\", -1)\n",
        "\n",
        "        # Validate feedback data\n",
        "        if not email or predicted_label == -1 or actual_label == -1:\n",
        "            return jsonify({\"error\": \"Invalid feedback data\"}), 400\n",
        "\n",
        "        # Log feedback for debugging\n",
        "        print(\"Received Feedback Data:\")\n",
        "        print(f\"Email: {email}\")\n",
        "        print(f\"Predicted Label: {predicted_label}\")\n",
        "        print(f\"Actual Label: {actual_label}\")\n",
        "\n",
        "        # Save feedback to CSV\n",
        "        feedback_data = pd.DataFrame([{\n",
        "            \"email\": email,\n",
        "            \"predicted_label\": predicted_label,\n",
        "            \"actual_label\": actual_label\n",
        "        }])\n",
        "        feedback_data.to_csv(FEEDBACK_FILE, mode=\"a\", header=not os.path.exists(FEEDBACK_FILE), index=False)\n",
        "\n",
        "        # Check feedback count and retrain if needed\n",
        "        feedback_count = len(pd.read_csv(FEEDBACK_FILE))\n",
        "        print(f\"Feedback count: {feedback_count}\")\n",
        "        if feedback_count >= 100:\n",
        "            retrain_model()\n",
        "\n",
        "        return jsonify({\"message\": \"Feedback submitted successfully\"})\n",
        "    except Exception as e:\n",
        "        print(f\"Error in feedback endpoint: {str(e)}\")\n",
        "        return jsonify({\"error\": f\"Error during feedback processing: {str(e)}\"}), 500\n",
        "\n",
        "def retrain_model():\n",
        "    print(\"\\nRetraining the model with feedback data...\")\n",
        "    feedback_data = pd.read_csv(FEEDBACK_FILE)\n",
        "    X = feedback_data['email']\n",
        "    y = feedback_data['actual_label']\n",
        "\n",
        "    # Perform retraining for one of the models (e.g., Logistic Regression)\n",
        "    vectorizer = TfidfVectorizer(max_features=5000, stop_words=\"english\", ngram_range=(1, 2))\n",
        "    X_tfidf = vectorizer.fit_transform(X)\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X_tfidf, y, test_size=0.2, random_state=42)\n",
        "    model = LogisticRegression(max_iter=1000, class_weight='balanced')\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    # Save the retrained model\n",
        "    joblib.dump(model, \"best_spam_classifier.pkl\")\n",
        "    print(\"\\nModel retrained and updated successfully.\")\n",
        "\n",
        "def start_server_and_tunnel():\n",
        "    dynamic_port = random.randint(8000, 9000)\n",
        "    threading.Thread(target=lambda: app.run(port=dynamic_port)).start()\n",
        "    public_url = ngrok.connect(dynamic_port).public_url\n",
        "    print(f\"Access the app at: {public_url}\")\n",
        "\n",
        "# Start the server\n",
        "start_server_and_tunnel()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "82wu_0uh7v66"
      },
      "outputs": [],
      "source": [
        "!pkill -f ngrok"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7LMrnoGnz3Ol"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4Q6sgC4E78Mi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "O32UFBIf78JO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OECntkFd78Gt"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}